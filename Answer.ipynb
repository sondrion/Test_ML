{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/MecAgent/mecagent-technical-test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w007HoJ02Ie4",
        "outputId": "7791ebff-e5d7-479f-ea5e-a10874671232"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'mecagent-technical-test' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mecagent-technical-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK9ClibY2c_v",
        "outputId": "3386f930-30f8-4de5-cad4-7768f563e9c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mecagent-technical-test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu739ASs1KvE"
      },
      "source": [
        "## ENV SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om43lEST1KvF"
      },
      "source": [
        "1. Install uv (or do it you're own way)\n",
        "2. Run `uv sync`\n",
        "3. Run `source .venv/bin/activate`\n",
        "\n",
        "You're good to go."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install uv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bZQkExe1QQ6",
        "outputId": "21152258-9fdd-45dc-e0ed-0df8667991df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uv in /usr/local/lib/python3.11/dist-packages (0.7.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "metadata": {
        "id": "9tl-NXdh1f2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc347c97-eff1-4600-cdf7-5b63dd4fa183"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 308\n",
            "-rw-r--r-- 1 root root    203 Jun 23 14:24 '=1.15.3'\n",
            "-rw-r--r-- 1 root root   1366 Jun 23 14:23 '=2.5.2'\n",
            "-rw-r--r-- 1 root root   3971 Jun 23 14:24 '=3.6.0'\n",
            "-rw-r--r-- 1 root root    200 Jun 23 14:24 '=4.6.11'\n",
            "-rw-r--r-- 1 root root   3361 Jun 23 14:24 '=6.29.5'\n",
            "drwxr-xr-x 4 root root   4096 Jun 23 13:57  CAD-Coder\n",
            "-rw-r--r-- 1 root root   4603 Jun 23 12:52  good_luck.ipynb\n",
            "drwxr-xr-x 2 root root   4096 Jun 23 12:52  metrics\n",
            "-rw-r--r-- 1 root root    281 Jun 23 12:52  pyproject.toml\n",
            "-rw-r--r-- 1 root root     56 Jun 23 12:52  README.md\n",
            "-rw-r--r-- 1 root root 268109 Jun 23 12:52  uv.lock\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install toml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBHgG-W09M2Q",
        "outputId": "26679309-3eaa-4eda-9228-10a102852cbb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (0.10.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import toml\n",
        "\n",
        "with open(\"pyproject.toml\", \"r\") as f:\n",
        "    pyproject = toml.load(f)\n",
        "\n",
        "if \"project\" in pyproject:\n",
        "    deps = pyproject[\"project\"].get(\"dependencies\", [])\n",
        "elif \"tool\" in pyproject and \"poetry\" in pyproject[\"tool\"]:\n",
        "    poetry_deps = pyproject[\"tool\"][\"poetry\"][\"dependencies\"]\n",
        "    deps = [pkg + (f\"{ver}\" if isinstance(ver, str) else \"\") for pkg, ver in poetry_deps.items() if pkg != \"python\"]\n",
        "else:\n",
        "    deps = []\n",
        "\n",
        "print(f\"Dépendances détectées : {deps}\")\n",
        "\n",
        "for dep in deps:\n",
        "    print(f\"Installation : {dep}\")\n",
        "    !pip install {dep}"
      ],
      "metadata": {
        "id": "nQpC_omd10eX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fa6944-f04e-4ced-d8d4-95e724e8533e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dépendances détectées : ['cadquery>=2.5.2', 'datasets>=3.6.0', 'ipykernel>=6.29.5', 'scipy>=1.15.3', 'trimesh>=4.6.11']\n",
            "Installation : cadquery>=2.5.2\n",
            "Installation : datasets>=3.6.0\n",
            "Installation : ipykernel>=6.29.5\n",
            "Installation : scipy>=1.15.3\n",
            "Installation : trimesh>=4.6.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t3mmx2W1KvG"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "The Task : Create the best CadQuery code generator model.\n",
        "\n",
        "1. Load the dataset (147K pairs of Images/CadQuery code).\n",
        "2. Create a baseline model and evaluate it with the given metrics.\n",
        "3. Enhance by any manner the baseline model and evaluate it again.\n",
        "4. Explain you choices and possible bottlenecks.\n",
        "5. Show what enhancements you would have done if you had more time.\n",
        "\n",
        "You can do *WHATEVER* you want, be creative, result is not what matters the most.\n",
        "Creating new model architectures, reusing ones you used in the past, fine-tuning, etc...\n",
        "\n",
        "If you are GPU poor, there are solutions. Absolute value is not what matters, relative value between baseline and enhanced model is what matters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBXdeUsMFB5B",
        "outputId": "de79f021-3931-4d77-d3fa-a76c3cfa9fcf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Ce code étant assez long, j'ai essayé avec 100 seulement !"
      ],
      "metadata": {
        "id": "K8rwJF_KKeDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ~/.cache/huggingface/datasets\n"
      ],
      "metadata": {
        "id": "zd1Yv80AMTnz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qvj3eVloMWGs",
        "outputId": "67bbc992-7ed8-4291-ab07-7e0e9fea3cb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "datasets",
                  "fsspec"
                ]
              },
              "id": "7785717b27524a968d886e21d21148dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_stream = load_dataset(\"ag_news\", split=\"train\", streaming=True)\n",
        "\n",
        "from itertools import islice\n",
        "\n",
        "subset_100 = list(islice(dataset_stream, 100))\n",
        "\n",
        "print(f\"Taille subset : {len(subset_100)}\")\n",
        "print(subset_100[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M0cxm1rMyvO",
        "outputId": "dc05d02b-9c70-405d-f8dc-803baa9040e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille subset : 100\n",
            "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enhanced Baseline Model (PyTorch)**"
      ],
      "metadata": {
        "id": "jwkv0D0sO81g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
        "\n",
        "# Simple pretrained ResNet encoder for images\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]  # Remove last FC layer\n",
        "        self.feature_extractor = nn.Sequential(*modules)\n",
        "        self.fc = nn.Linear(resnet.fc.in_features, embed_dim)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.feature_extractor(images).squeeze()  # B x 512\n",
        "        embeddings = self.fc(features)  # B x embed_dim\n",
        "        return embeddings.unsqueeze(1)  # B x 1 x embed_dim\n",
        "\n",
        "# Transformer Decoder for code generation\n",
        "class CodeDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=512, nhead=8, num_layers=6):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model=embed_dim, nhead=nhead)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, tgt_seq, memory):\n",
        "        # tgt_seq: (T, B)\n",
        "        tgt_emb = self.embedding(tgt_seq) * (self.embed_dim ** 0.5)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq.size(0)).to(tgt_seq.device)\n",
        "        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
        "        logits = self.fc_out(output)\n",
        "        return logits\n",
        "\n",
        "# Combined model\n",
        "class EnhancedModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=512):\n",
        "        super().__init__()\n",
        "        self.encoder = ImageEncoder(embed_dim)\n",
        "        self.decoder = CodeDecoder(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, images, tgt_seq):\n",
        "        memory = self.encoder(images)  # B x 1 x embed_dim\n",
        "        memory = memory.permute(1, 0, 2)  # 1 x B x embed_dim for Transformer\n",
        "        tgt_seq = tgt_seq.permute(1, 0)  # T x B\n",
        "        output = self.decoder(tgt_seq, memory)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Hyperparams\n",
        "VOCAB_SIZE = 10000  # Your tokenizer vocab size\n",
        "EMBED_DIM = 512\n",
        "\n",
        "model = EnhancedModel(VOCAB_SIZE, EMBED_DIM)\n",
        "\n",
        "# Dummy inputs\n",
        "batch_size = 4\n",
        "image = torch.randn(batch_size, 3, 224, 224)  # Images\n",
        "target_seq = torch.randint(0, VOCAB_SIZE, (20, batch_size))  # Target token ids (seq_len=20)\n",
        "\n",
        "# Forward pass\n",
        "logits = model(image, target_seq)\n",
        "\n",
        "# Compute loss (CrossEntropy + syntactic loss example)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # assuming 0 is padding idx\n",
        "\n",
        "# Flatten logits and targets for loss\n",
        "logits_flat = logits.view(-1, VOCAB_SIZE)\n",
        "target_flat = target_seq.reshape(-1)\n",
        "\n",
        "loss_ce = criterion(logits_flat, target_flat)\n",
        "\n",
        "# Example: Add weight for syntax-valid sequences (placeholder)\n",
        "syntax_valid_weight = 1.2\n",
        "loss = loss_ce * syntax_valid_weight\n",
        "\n",
        "loss.backward()\n"
      ],
      "metadata": {
        "id": "mcLPyDR8PZDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanations :  Why these enhancements?\n",
        "\n",
        "Pretrained models provide strong prior knowledge, improving code generation quality.\n",
        "\n",
        "Multi-task learning injects structural knowledge about CAD objects, helping model understand geometry better.\n",
        "\n",
        "Data augmentation combats overfitting and increases robustness.\n",
        "\n",
        "Bottlenecks:\n",
        "\n",
        "Compute resources: Large models require significant GPU memory and training time.\n",
        "\n",
        "Data quality: Errors or inconsistencies in dataset labels/code degrade model performance.\n",
        "\n",
        "Evaluation time: Mesh IOU requires generating and comparing 3D models, which is computationally heavy.\n",
        "\n",
        "Syntax checking: Runtime evaluation of code validity can be slow and prone to crashes if not sandboxed."
      ],
      "metadata": {
        "id": "07MxA7DxOZR0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4XZhoFn1KvH"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfAQe58a1KvI"
      },
      "source": [
        "1. Valid Syntax Rate metric assess the validity of the code by executing and checking if error are returned.\n",
        "2. Best IOU assess the similarity between the meshes generated by the code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
        "from metrics.best_iou import get_iou_best"
      ],
      "metadata": {
        "id": "vTk_EgN2Jl06"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_code = \"\"\"\n",
        "height = 60.0\n",
        "width = 80.0\n",
        "thickness = 10.0\n",
        "diameter = 22.0\n",
        "\n",
        "# make the base\n",
        "result = (\n",
        "    cq.Workplane(\"XY\")\n",
        "    .box(height, width, thickness)\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "sample_code_2 = \"\"\"\n",
        " height = 60.0\n",
        " width = 80.0\n",
        " thickness = 10.0\n",
        " diameter = 22.0\n",
        " padding = 12.0\n",
        "\n",
        " # make the base\n",
        " result = (\n",
        "     cq.Workplane(\"XY\")\n",
        "     .box(height, width, thickness)\n",
        "     .faces(\">Z\")\n",
        "     .workplane()\n",
        "     .hole(diameter)\n",
        "     .faces(\">Z\")\n",
        "     .workplane()\n",
        "     .rect(height - padding, width - padding, forConstruction=True)\n",
        "     .vertices()\n",
        "     .cboreHole(2.4, 4.4, 2.1)\n",
        " )\n",
        "\"\"\"\n",
        "\n",
        "codes = {\n",
        "    \"sample_code\": sample_code,\n",
        "    \"sample_code_2\": sample_code_2,\n",
        "}\n",
        "\n",
        "vsr = evaluate_syntax_rate_simple(codes)\n",
        "print(\"Valid Syntax Rate:\", vsr)\n",
        "\n",
        "iou = get_iou_best(sample_code, sample_code_2)\n",
        "print(\"IOU:\", iou)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvfdocxCJveD",
        "outputId": "b7bc8e7f-b078-4ca6-d815-31c1ae105442"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Syntax Rate: 1.0\n",
            "IOU: 0.5834943417057687\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}